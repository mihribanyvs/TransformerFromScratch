{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Bring in PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Most of the examples have typing on the signatures for readability\n",
    "from typing import Optional, Callable, List, Tuple\n",
    "from Bio import SeqIO\n",
    "# For data loading\n",
    "from torch.utils.data import Dataset, IterableDataset, TensorDataset, DataLoader\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "import bz2\n",
    "\n",
    "# For progress and timing\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import shutil\n",
    "from Bio.PDB import PDBList\n",
    "from Bio.PDB.MMCIFParser import MMCIFParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (171553503.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    rom Bio.PDB import PDBList\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "rom Bio.PDB import PDBList\n",
    "\n",
    "#def pdb_retriever(file_name):\n",
    "file_name = 'AF-A0A1D8PD42-F1-model_v4'\n",
    "#file_name = '3goe'\n",
    "\n",
    "pdbl = PDBList()\n",
    "#pdbl.retrieve_pdb_file(\"3goe\", file_format='mmCif', pdir=\".\")\n",
    "pdbl.retrieve_pdb_file(file_name, file_format='mmCif', pdir=\".\")\n",
    "# import the needed class\n",
    "from Bio.PDB.MMCIFParser import MMCIFParser\n",
    "\n",
    "# instantiate the class to prepare the parser\n",
    "cif_parser = MMCIFParser()\n",
    "# load the structure to an object\n",
    "structure = cif_parser.get_structure(file_name, f\"{file_name}.cif\")\n",
    "#structure = cif_parser.get_structure(\"3goe\", \"3goe.cif\")\n",
    "\n",
    "model0 = structure[0]\n",
    "\n",
    "#model0 = structure[1] - error due to there only being one model\n",
    "chain_A = model0['A']  # and we get chain A\n",
    "# dictionary converting 3-letter codes to 1-letter codes\n",
    "# this is a very common need in bioinformatics of proteins\n",
    "d3to1 = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',\n",
    " 'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N',\n",
    " 'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W',\n",
    " 'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}\n",
    "\n",
    "seq = []\n",
    "for residue in chain_A:\n",
    "    # for simplicity we can use X for heteroatoms (ions and water)\n",
    "    seq.append(d3to1.get(residue.get_resname(), 'X'))  #converts water and ions to X\n",
    "print(''.join(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define standard amino acids\n",
    "AAs= \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "#define additional tokens for special purposes\n",
    "Additional_Tokens= ['<OTHER>', '<START>', '<END>', '<PAD>']\n",
    "\n",
    "#number of additional tokens added to each sequence (start and end)\n",
    "added_tokens_per_seq = 2\n",
    "\n",
    "#number of amino acids\n",
    "n_AAs= len(AAs)\n",
    "\n",
    "#create a dictionary mapping each amino acid to a unique index\n",
    "aa_to_token_index= {aa: i for i, aa in enumerate(AAs)}\n",
    "\n",
    "#create a dictionary mapping each additional token to a unique index starting after the amino acids\n",
    "additional_token_to_index= {token: i + n_AAs for i, token in enumerate(Additional_Tokens)}\n",
    "\n",
    "#combine the two dictionaries into one for easy tokens indexing\n",
    "token_to_index= {**aa_to_token_index, **additional_token_to_index}\n",
    "\n",
    "#create a reverse dictionary mapping indices to tokens\n",
    "index_to_token= {index: token for token, index in token_to_index.items()}\n",
    "\n",
    "#total number of tokens (amino acids + additional tokens)\n",
    "n_token= len(token_to_index)\n",
    "\n",
    "def tokenize_seq(seq):\n",
    "    \"\"\"\n",
    "    Convert a sequence into token indices.\n",
    "\n",
    "    :param seq: The input sequence (string, bytes, or SeqRecord)\n",
    "    :return: A tensor of token indices\n",
    "    \"\"\"\n",
    "    #Index for the <OTHER> token used for unknown amino acids\n",
    "    other_token_index= additional_token_to_index[\"<OTHER>\"]\n",
    "\n",
    "    #Tokenize the sequence with start and end tokens\n",
    "    tokenized_seq= [additional_token_to_index[\"<START>\"]] + [aa_to_token_index.get(aa, other_token_index) for aa in parse_seq(seq)] + [additional_token_to_index[\"<END>\"]]\n",
    "    \n",
    "    #convert tokenized sequence into a tensor\n",
    "    return torch.tenosor(tokenized_seq)\n",
    "\n",
    "def parse_seq(seq):\n",
    "    \"\"\"\n",
    "    Parse the input sequence into a string.\n",
    "\n",
    "    :param seq: The input sequence which can be a string, bytes, or SeqRecord\n",
    "    :return: A string representation of the sequence\n",
    "    \"\"\"\n",
    "    if isinstance(seq, str):\n",
    "        # If the sequence is already a string, return it as is\n",
    "        return seq\n",
    "    elif isinstance(seq, bytes):\n",
    "        # If the sequence is in bytes, decode it to a string using utf-8 encoding\n",
    "        return seq.decode(\"utf8\")\n",
    "    elif isinstance(seq, SeqIO.SeqRecord):\n",
    "        # If the sequence is a SeqRecord, extract the sequence part as a string\n",
    "        return str(seq.seq)\n",
    "    else:\n",
    "        # Raise a TypeError if the input sequence type is not recognized\n",
    "        raise TypeError(\"unexpected sequence type: %s' % type(seq)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding layer that combines tokens embeddings with positional encodings\n",
    "    \"\"\"\n",
    "    def __init__(self, n_token, embedding_dim):\n",
    "        \"\"\"\n",
    "        :param n_token: total number of unique token\n",
    "        :param embedding_dim: dimension of the embedding space\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        # Token embedding layer\n",
    "        self.token_embedding= nn.Embedding(n_token, embedding_dim)\n",
    "        # positional encoding layer\n",
    "        self.positional_encoding= PositionalEncoding(embedding_dim)\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass for the embedding layer.\n",
    "\n",
    "        :param input_seq: Input sequence of token indices\n",
    "        :return: Combined embeddings with positional encodings\n",
    "        \"\"\"\n",
    "        token_embeddings= self.token_embedding(input_seq)\n",
    "        positional_encodings= self.positional_encoding(input_seq)\n",
    "        embeddings= token_embeddings + positional_encodings\n",
    "        return embeddings\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding layer to add positional information to token embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, max_len= 100):\n",
    "        \"\"\"\n",
    "        Initializes the positional encoding layer.\n",
    "\n",
    "        :param embedding_dim: Dimension of the embedding space\n",
    "        :param max_len: Maximum length of the sequences to be encoded\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        #create a matrix of position (0 to max_len-1) with shape [max_len, 1]\n",
    "        position= torch.arange(0, max_len).unsqueeze(1)\n",
    "        #compute the division term for the position encodings\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * -(np.log(10000.0) / embedding_dim))\n",
    "        #initialize a matrix for positional encodings with shape [max_len, 1, embedding_dim]\n",
    "        pe = torch.zeros(max_len, 1, embedding_dim)\n",
    "        #apply sine to even indices in the array\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        #apply cosine to odd indices in the array\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        forward pass for the positional encoding layer\n",
    "        :param input_seq: input tensor of shape [batch_size, seq_len]\n",
    "        :return: positional encodings corresponding to the input sequence length\n",
    "        \"\"\"\n",
    "        #get the positional encodings up to the length of the input sequence\n",
    "        return self.pe[:input_seq.size(1), :].transpose(0, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param embedding_dim: dimension of input embeddings\n",
    "        :param num_heads: number of attention heads\n",
    "        \"\"\"\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        assert embedding_dim % num_heads == 0 #embedding dimension must be divisible by nember of heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads= num_heads\n",
    "        self.d_k = embedding_dim // num_heads #dimension of each attention head\n",
    "\n",
    "        #linear layers to project input embeddings to queries, keys, and values\n",
    "        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_o = nn.Linear(embedding_dim, embedding_dim) #linear layer to combine the heads' outputs\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        :param Q: query tensor\n",
    "        :param K: key tensor\n",
    "        :param V: value tensor\n",
    "        :param mask: optional mask tensor \n",
    "        :return: attention output tensor\n",
    "        \"\"\"\n",
    "        #calculate the attention score\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        #apply softmax to egt the attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        #compute the attention output as a weighted sum of the values\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        split the input tensor into multiple heads.\n",
    "\n",
    "        :param x: input tensor of shape (batch_size, input_seq, embedding_dim)\n",
    "        :return: tensor of shape (batch_size, num_heads, input_seq, d_k)\n",
    "        \"\"\"\n",
    "        batch_size, input_seq, embedding_dim = x.size()\n",
    "        #reshape and transpose to seperate heads \n",
    "        return x.view(batch_size, input_seq, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        combine multiple heads back into a single tensor \n",
    "        \n",
    "        :param x: input tensor of shape (batch_size, num_heads, input_seq, d_k)\n",
    "        :return: tensor of shape (batch_size, input_seq, embedding_dim)\n",
    "        \"\"\"\n",
    "        batch_size, _, input_seq, d_k = x.size()\n",
    "        #transpose and reshape to combine heads\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, input_seq, self.embedding_dim)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask= None):\n",
    "        \"\"\"\n",
    "        Forward pass for the MultiheadAttention module.\n",
    "\n",
    "        :param Q: Query tensor\n",
    "        :param K: Key tensor\n",
    "        :param V: Value tensor\n",
    "        :param mask: Optional mask tensor\n",
    "        :return: Output tensor of the multihead attention mechanism\n",
    "        \"\"\"\n",
    "        Q= self.split_heads(self.w_q(Q))\n",
    "        K= self.split_heads(self.w_k(K))\n",
    "        V= self.split_heads(self.w_v(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V)\n",
    "        output= self.w_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        inp_ch: int,\n",
    "        inp_seq: int, \n",
    "        key_dim: list, \n",
    "        que_dim: list, \n",
    "        out_ch: int, \n",
    "        out_seq: int, \n",
    "        d_k: int, \n",
    "        device, \n",
    "        verbose=False) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the SingleHeadAttention module.\n",
    "        \n",
    "        :param inp_ch: Number of input channels (N)\n",
    "        :param inp_seq: Input sequence length (T)\n",
    "        :param key_dim: List containing dimensions for the key projection matrix (k, s)\n",
    "        :param que_dim: List containing dimensions for the query projection matrix (k, u)\n",
    "        :param out_ch: Number of output channels (m)\n",
    "        :param out_seq: Output sequence length (o)\n",
    "        :param d_k: Dimension for scaling in dot-product attention\n",
    "        :param device: Device to perform computations on (CPU/GPU)\n",
    "        :param verbose: Flag for printing intermediate shapes for debugging\n",
    "        \"\"\"\n",
    "        super(SingleHeadAttention, self).__init__()  \n",
    "\n",
    "        self.N = inp_ch #number of input channels\n",
    "        self.T = inp_seq #input sequence length\n",
    "        self.k, self.s = key_dim #dimensions for the key projection matrix\n",
    "        self.k, self.u = que_dim #dimensions for the query projectoin matrix\n",
    "        self.m = out_ch #number of output channels\n",
    "        self.o = out_seq #output sequence length\n",
    "        self.d_k = d_k #dimension for scaling in dot-product attention\n",
    "        self.device = device #device to perform computations on\n",
    "        \n",
    "        #initialize learnable weight parameters for q, k, and v matrices\n",
    "        self.w_q = nn.Parameter(torch.randn([self.s, self.T]))\n",
    "        self.w_k = nn.Parameter(torch.randn([self.u, self.o]))\n",
    "        self.w_v = nn.Parameter(torch.randn([self.m, self.N]))\n",
    "        self.verbos = verbose #flag for printing intermediate shapes for debugging\n",
    "     \n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        \"\"\"\n",
    "        Forward pass for the SingleHeadAttention module.\n",
    "        \n",
    "        :param Q: Query tensor\n",
    "        :param K: Key tensor\n",
    "        :param V: Value tensor\n",
    "        :return: Tuple of (context, attention weights)\n",
    "        \"\"\"\n",
    "        #project the input tensors q, k, and v using the corresponding weight matrices\n",
    "        Q = torch.matmul(Q, self.w_q.to(self.device))\n",
    "        K = torch.matmul(K, self.w_k.to(self.device))\n",
    "        V = torch.matmul(V, self.w_v.to(self.device))\n",
    "        \n",
    "        #compute the attention scores by by scaling the dot product of q and k\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
    "        #apply the softmax function to obtain the attention weights \n",
    "        attn = torch.softmax(scores, dim=-1)  \n",
    "        # Compute the context vector as the weighted sum of the values V\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_token, \n",
    "        embedding_dim,\n",
    "        output_dim = 2, \n",
    "        num_heads, \n",
    "        feed_forward_dim, \n",
    "        dropout_rate= 0.1,\n",
    "        max_len= 100):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        #Embedding layer: maps each token to a dense vector of fixed size (embedding dim)\n",
    "        self.embedding= nn.Embedding(n_token, embedding_dim) \n",
    "        #positional encoding layer: adds positional information to the token embeddings \n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim, max_len)\n",
    "        #single head self attention layer: computee attention scores and context vectors \n",
    "        self.self_attention = SingleHeadAttention(\n",
    "            inp_ch=embedding_dim, \n",
    "            inp_seq=max_len,\n",
    "            key_dim=[embedding_dim // num_heads, embedding_dim],\n",
    "            que_dim=[embedding_dim // num_heads, embedding_dim],\n",
    "            out_ch=embedding_dim,\n",
    "            out_seq=max_len,\n",
    "            d_k=embedding_dim // num_heads,\n",
    "            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "            verbose=False\n",
    "        )\n",
    "        #layer normalization layers: normalize the outputs of each sub-layer\n",
    "        self.layer_norm1= nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2= nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        #feed-forward network: three linear layers with ReLU activation in between\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, feed_forward_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feed_forward_dim, feed_forward_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feed_forward_dim, output_dim)\n",
    "        )\n",
    "        #dropout layer: randomly zeroes some of the elements of the input tensor with probability dropout_rate\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        #compute token embeddings\n",
    "        token_embeddings = self.embedding(input_seq)\n",
    "        #compute positional encodings\n",
    "        positional_encoding= self.positional_encoding(input_seq)\n",
    "        embeddings = token_embeddings + positional_encoding\n",
    "\n",
    "        #self_attention\n",
    "        attn_output, _ = self.self_attention(embeddings, embeddings, embeddings)\n",
    "        #apply dropout to attention output \n",
    "        attn_output = self.dropout(attn_output)\n",
    "        #add residual connection and apply layer normalizaiton\n",
    "        out1 = self.layer_norm1(embeddings + attn_output)\n",
    "\n",
    "        #apply feed-forward network \n",
    "        ffn_output = self.feed_forward(out1)\n",
    "        #apply dropout to feed-foward output\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "        #add residual connection and apply layer normalization\n",
    "        out2 = self.layer_norm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_token,\n",
    "        embedding_dim,\n",
    "        output_dim,\n",
    "        num_heads,\n",
    "        feed_forward_dim,\n",
    "        num_layers=1,\n",
    "        dropout_rate=0.1,\n",
    "        max_len= 100\n",
    "    ):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(n_token, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model = embedding_dim,\n",
    "            nhead = num_heads,\n",
    "            dim_feedforward=feed_forward_dim,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embedding_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        #stack multiple transformer encoder layers\n",
    "        #each layer is an instance of the TransformerEncoderLayer class\n",
    "        #self.layers = nn.ModuleList([TransformerEncoderLayer(n_token, embedding_dim, output_dim, num_heads, feed_forward_dim, dropout_rate, max_len) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.dropout(embedded)\n",
    "        encoded = self.transformer_encoder(embedded)\n",
    "        output = self.fc_out(encoded)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 1024  # Dimensionality of the input embedding for each amino acid\n",
    "feed_forward_dim = 512  # Hidden size of the transformer \n",
    "num_layers = 6  # Number of encoder layers \n",
    "num_heads = 8  # Number of attention heads \n",
    "output_dim = 2  # Dimensionality of the output (phi and psi angles)\n",
    "dropout_rate= 0.01\n",
    "max_len = 128\n",
    "\n",
    "model = TransformerEncoder(n_token, embedding_dim, output_dim, num_heads, feed_forward_dim, num_layers, dropout_rate, max_len)\n",
    "model = model.to(device)\n",
    "\n",
    "#define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=additional_token_to_index[\"<PAD>\"]) #use <PAD> token index as ignore index\n",
    "optimizer = optim.Adam(model.parameter(), lr=0.001)\n",
    "\n",
    "def train(model, test_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for input_seq, target_seq in test_loader:\n",
    "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "\n",
    "        #clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_seq = model(input_seq)\n",
    "\n",
    "        #reshape output and target to calculate loss\n",
    "        output_seq = output_seq.view(-1, n_token)\n",
    "        target_seq = target_seq.view(-1)\n",
    "\n",
    "        #calculate loss: softmax --> cross entropy loss\n",
    "        loss = criterion(output_seq, target_seq)\n",
    "\n",
    "        #backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss +=loss.item()\n",
    "    \n",
    "    return total_loss / len(test_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train(model, test_loader, criterion, optimizer, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_token = N\n",
    "embedding_dim = D\n",
    "output_dim = 2\n",
    "num_heads = 1\n",
    "feed_forward_dim = 512\n",
    "num_layers = 2\n",
    "dropout_rate = 0.001\n",
    "max_len = 128\n",
    "model = TransformerEncoder(n_token, embedding_dim, output_dim, num_heads, feed_forward_dim, num_layers, dropout_rate, max_len)\n",
    "# model = TransformerEncoder(N, D, 2, 1, 200)\n",
    "model = model.to(device)\n",
    "num_epochs = 100\n",
    "\n",
    "#define the loss function and optimizer\n",
    "criterion = nn.MSELoss()#nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) #model.parameter?\n",
    "\n",
    "def train(model, data, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for input_seq, target_seq in data:\n",
    "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "\n",
    "        #clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_seq = model(input_seq)\n",
    "\n",
    "        #reshape output and target to calculate loss\n",
    "        output_seq = output_seq.view(-1, N)\n",
    "        target_seq = target_seq.view(-1)\n",
    "\n",
    "        #calculate loss: softmax --> cross entropy loss\n",
    "        loss = criterion(output_seq, target_seq)\n",
    "\n",
    "        #backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss +=loss.item()\n",
    "\n",
    "    return total_loss / len(data)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train(model, data, criterion, optimizer, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
