{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Bring in PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.w_q = nn.Parameter(torch.randn(embed_dim, embed_dim))\n",
    "        self.w_k = nn.Parameter(torch.randn(embed_dim, embed_dim))\n",
    "        self.w_v = nn.Parameter(torch.randn(embed_dim, embed_dim))\n",
    "\n",
    "    def forward(self, embeddings_prot_bert: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        Q = torch.matmul(embeddings_prot_bert, self.w_q)\n",
    "        K = torch.matmul(embeddings_prot_bert, self.w_k)\n",
    "        V = torch.matmul(embeddings_prot_bert, self.w_v)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(K.size(-1))\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf')) #if masked, the value is going as low as possible to avoid being scored\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attention_output = torch.matmul(attn, V)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_after_train(nn.Module):\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super(SelfAttention_after_train, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.w_q = nn.Parameter(torch.randn(embed_dim, embed_dim))\n",
    "        self.w_k = nn.Parameter(torch.randn(embed_dim, embed_dim))\n",
    "        self.w_v = nn.Parameter(torch.randn(embed_dim, embed_dim))\n",
    "\n",
    "    def forward(self, embeddings_prot_bert: torch.Tensor) -> torch.Tensor:\n",
    "        Q = torch.matmul(embeddings_prot_bert, self.w_q)\n",
    "        K = torch.matmul(embeddings_prot_bert, self.w_k)\n",
    "        V = torch.matmul(embeddings_prot_bert, self.w_v)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(K.size(-1))\n",
    "        \n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attention_output = torch.matmul(attn, V)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(indices ): #lengths can be df['len']\n",
    "    mask = torch.arange(131) < lengths[indices] #maybe we need to unsqueeze this\n",
    "    mask = mask.to(device)\n",
    "    return mask\n",
    "           #[0,1,...130]\n",
    "\n",
    "class AngularLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AngularLoss, self).__init__()\n",
    "\n",
    "    def forward(self,idx, predicted_angles, angles_tensor):\n",
    "\n",
    "        predicted_angles_phi, predicted_angles_psi = predicted_angles[:, 0], predicted_angles[:, 1]\n",
    "        angles_tensor_phi, angles_tensor_psi = angles_tensor[:,0, idx], angles_tensor[:, 1,idx]\n",
    "        \n",
    "        predicted_angles_phi = (predicted_angles_phi + torch.pi) % (2 * torch.pi) - torch.pi\n",
    "        angles_tensor_phi = (angles_tensor_phi + torch.pi) % (2 * torch.pi) - torch.pi\n",
    "        predicted_angles_psi = (predicted_angles_psi + torch.pi) % (2 * torch.pi) - torch.pi\n",
    "        angles_tensor_psi = (angles_tensor_psi + torch.pi) % (2 * torch.pi) - torch.pi\n",
    "\n",
    "        difference_phi = torch.abs(predicted_angles_phi - angles_tensor_phi)*create_mask(indices= idx)\n",
    "        loss_phi = torch.mean(torch.min(difference_phi, 2 * torch.pi - difference_phi)) / create_mask(indices= idx).sum()\n",
    "\n",
    "        difference_psi = torch.abs(predicted_angles_psi - angles_tensor_psi)*create_mask(indices= idx)\n",
    "        loss_psi = torch.mean(torch.min(difference_psi, 2 * torch.pi - difference_psi))/ create_mask(indices= idx).sum()\n",
    "\n",
    "        loss = loss_phi + loss_psi\n",
    "        #print(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, embed_dim: int, feed_forward_dim1: int, feed_forward_dim2: int, output_dim: int = 2, dropout_rate: float = 0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.self_attention = SelfAttention(embed_dim)\n",
    "        self.self_attention_after_train = SelfAttention_after_train(embed_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.feed_forward = nn.Sequential(  \n",
    "            nn.Linear(embed_dim, feed_forward_dim1),\n",
    "            nn.GELU(),\n",
    "            self.dropout,\n",
    "            nn.Linear(feed_forward_dim1, feed_forward_dim2),\n",
    "            nn.GELU(),\n",
    "            self.dropout,\n",
    "            nn.Linear(feed_forward_dim2, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        attention_output = self.self_attention(embeddings, mask)\n",
    "        normalized_attention_output = self.layer_norm1(attention_output)\n",
    "        ff_output = self.feed_forward(normalized_attention_output)\n",
    "        output = self.layer_norm2(ff_output)\n",
    "        return ff_output\n",
    "    \n",
    "    def forward_after_train(self, embeddings: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        attention_output = self.self_attention_after_train(embeddings)\n",
    "        normalized_attention_output = self.layer_norm1(attention_output)\n",
    "        ff_output = self.feed_forward(normalized_attention_output)\n",
    "        output = self.layer_norm2(ff_output)\n",
    "        return ff_output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
