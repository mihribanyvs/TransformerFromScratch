{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = \"M S S S N T D N Q Y P K Y I N D T T P P T I T L K E Y D N A S W A S T T C L D H N P I K N Q Y I V V V M E N P N Q I V A I I D Q Q D N M I L D I L F K N A H D A H S K Q E Y S T K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import re\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "seq = re.sub(r\"[UZOB]\", \"X\", seq)\n",
    "encoded_input = tokenizer(seq, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Bring in PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Most of the examples have typing on the signatures for readability\n",
    "from typing import Optional, Callable, List, Tuple\n",
    "from Bio import SeqIO\n",
    "# For data loading\n",
    "from torch.utils.data import Dataset, IterableDataset, TensorDataset, DataLoader\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "import bz2\n",
    "\n",
    "# For progress and timing\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import shutil\n",
    "from Bio.PDB import PDBList\n",
    "from Bio.PDB.MMCIFParser import MMCIFParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "\n",
    "seq_example = re.sub(r\"[UZOB]\", \"X\", seq_example)\n",
    "encoded_input = tokenizer(seq_example, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(input_sequence, feed_forward_dim, output_dim=2):\n",
    "    #putting the input in the embedding space\n",
    "    model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "    \n",
    "    output = model(**input_sequence)\n",
    "    embedding_prot_bert = output.last_hidden_state\n",
    "    N , D = embedding_prot_bert.size()[1], embedding_prot_bert.size()[2]\n",
    "\n",
    "    #self, single-head, unmasked attention layer\n",
    "    #attention weights\n",
    "    w_q = nn.Parameter(torch.randn([D, D])) #(DxD)\n",
    "    w_k = nn.Parameter(torch.randn([D, D])) #(DxD)\n",
    "    w_v = nn.Parameter(torch.randn([D, D])) #(DxD)\n",
    "\n",
    "    Q = torch.matmul(embedding_prot_bert, w_q)\n",
    "    K = torch.matmul(embedding_prot_bert, w_k)\n",
    "    V = torch.matmul(embedding_prot_bert, w_v)\n",
    "\n",
    "    scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(K.size()[1])\n",
    "    #apply the softmax function to obtain the attention weights\n",
    "    attn = torch.softmax(scores, dim=-1)\n",
    "    # Compute the context vector as the weighted sum of the values V\n",
    "    attention_output = torch.matmul(attn, V) #(NxD)\n",
    "\n",
    "    #first normalization layer\n",
    "    layer_norm1 = nn.LayerNorm((N,D))\n",
    "    embedded_after_attention = layer_norm1(attention_output)\n",
    "\n",
    "    #feed forward neural network\n",
    "    feed_forward = nn.Sequential(\n",
    "        nn.Linear(D, feed_forward_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(feed_forward_dim, feed_forward_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(feed_forward_dim, output_dim) \n",
    "        )\n",
    "    \n",
    "    ff_output = feed_forward(embedded_after_attention) #(Nx2)\n",
    "\n",
    "    #second normalization layer\n",
    "    layer_norm2 = nn.LayerNorm((N,2))\n",
    "    embedded_after_ff = layer_norm2(ff_output)\n",
    "\n",
    "    return embedded_after_ff\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = encoded_input\n",
    "output = transformer(input_sequence,500)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
