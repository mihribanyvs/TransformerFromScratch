{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/content/AF-A0A1D8PD42-F1-model_v4.cif\"\n",
    "file_model = \"AF-A0A1D8PD42-F1-model_v4\"\n",
    "pdbl = PDBList()\n",
    "pdbl.retrieve_pdb_file(file_path, file_format='mmCif', pdir=\".\")\n",
    "# import the needed class\n",
    "# instantiate the class to prepare the parser\n",
    "cif_parser = MMCIFParser()\n",
    "#structure = cif_parser.get_structure(\"3goe\", \"3goe.cif\")\n",
    "structure = cif_parser.get_structure(file_model, file_path)\n",
    "model0 = structure[0]\n",
    "chain_A = model0['A']  # and we get chain A\n",
    "# dictionary converting 3-letter codes to 1-letter codes\n",
    "# this is a very common need in bioinformatics of proteins\n",
    "d3to1 = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',\n",
    " 'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N',\n",
    " 'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W',\n",
    " 'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}\n",
    "\n",
    "seq = []\n",
    "for residue in chain_A:\n",
    "    # for simplicity we can use X for heteroatoms (ions and water)\n",
    "    seq.append(d3to1.get(residue.get_resname(), 'X'))  #converts water and ions to X\n",
    "print(''.join(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import PICIO, PDBIO\n",
    "from Bio import PDB\n",
    "from typing import TypedDict, Dict, Tuple\n",
    "structure.atom_to_internal_coordinates() # turns xyz coordinates into angles and bond lengths\n",
    "\n",
    "chain:PDB.Chain.Chain = list(structure.get_chains())[0]#iterator of chains, turns it into list, [0] first chain\n",
    "\n",
    "ic_chain: PDB.internal_coords.IC_Chain = chain.internal_coord #this access the internal chain coords of the chain object\n",
    "\n",
    "d: Dict[Tuple[PDB.internal_coords.AtomKey,\n",
    "              PDB.internal_coords.AtomKey,\n",
    "              PDB.internal_coords.AtomKey,\n",
    "              PDB.internal_coords.AtomKey],\n",
    "        PDB.internal_coords.Dihedron] = ic_chain.dihedra\n",
    "\n",
    "cnt = 1\n",
    "\n",
    "phi_angles_list = []\n",
    "psi_angles_list = []\n",
    "\n",
    "for key in d:\n",
    "    if key[0].akl[3] == 'N' and key[1].akl[3] == 'CA' and key[2].akl[3] == 'C' and key[3].akl[3] == 'N':\n",
    "        phi_angles_list.append(d[key].angle)\n",
    "    elif key[0].akl[3] == 'CA' and key[1].akl[3] == 'C' and key[2].akl[3] == 'N' and key[3].akl[3] == 'CA':\n",
    "        psi_angles_list.append(d[key].angle)\n",
    "\n",
    "\n",
    "structure.internal_to_atom_coordinates(verbose = True)\n",
    "io = PDBIO() #this is to write a pdb file again\n",
    "io.set_structure(structure)#set structure, the structure you wan tin the pdb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_angles_list.append(0)\n",
    "psi_angles_list.append(0)\n",
    "\n",
    "phi = np.asarray(phi_angles_list,dtype=np.float32)*(np.pi/180)\n",
    "psi = np.asarray(psi_angles_list,dtype=np.float32)*(np.pi/180)\n",
    "angles = np.vstack((psi,phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_example =  ' '.join(seq)\n",
    "seq_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = \"M S S S N T D N Q Y P K Y I N D T T P P T I T L K E Y D N A S W A S T T C L D H N P I K N Q Y I V V V M E N P N Q I V A I I D Q Q D N M I L D I L F K N A H D A H S K Q E Y S T K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import re\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "seq = re.sub(r\"[UZOB]\", \"X\", seq)\n",
    "encoded_input = tokenizer(seq, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Bring in PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Most of the examples have typing on the signatures for readability\n",
    "from typing import Optional, Callable, List, Tuple\n",
    "from Bio import SeqIO\n",
    "# For data loading\n",
    "from torch.utils.data import Dataset, IterableDataset, TensorDataset, DataLoader\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "import bz2\n",
    "\n",
    "# For progress and timing\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import shutil\n",
    "from Bio.PDB import PDBList\n",
    "from Bio.PDB.MMCIFParser import MMCIFParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "\n",
    "seq_example = re.sub(r\"[UZOB]\", \"X\", seq_example)\n",
    "encoded_input = tokenizer(seq_example, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(input_sequence, feed_forward_dim, output_dim=2):\n",
    "    #putting the input in the embedding space\n",
    "    model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "    \n",
    "    output = model(**input_sequence)\n",
    "    embedding_prot_bert = output.last_hidden_state\n",
    "    N , D = embedding_prot_bert.size()[1], embedding_prot_bert.size()[2]\n",
    "\n",
    "    #self, single-head, unmasked attention layer\n",
    "    #attention weights\n",
    "    w_q = nn.Parameter(torch.randn([D, D])) #(DxD)\n",
    "    w_k = nn.Parameter(torch.randn([D, D])) #(DxD)\n",
    "    w_v = nn.Parameter(torch.randn([D, D])) #(DxD)\n",
    "\n",
    "    Q = torch.matmul(embedding_prot_bert, w_q)\n",
    "    K = torch.matmul(embedding_prot_bert, w_k)\n",
    "    V = torch.matmul(embedding_prot_bert, w_v)\n",
    "\n",
    "    scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(K.size()[1])\n",
    "    #apply the softmax function to obtain the attention weights\n",
    "    attn = torch.softmax(scores, dim=-1)\n",
    "    # Compute the context vector as the weighted sum of the values V\n",
    "    attention_output = torch.matmul(attn, V) #(NxD)\n",
    "\n",
    "    #first normalization layer\n",
    "    layer_norm1 = nn.LayerNorm((N,D))\n",
    "    embedded_after_attention = layer_norm1(attention_output)\n",
    "\n",
    "    #feed forward neural network\n",
    "    feed_forward = nn.Sequential(\n",
    "        nn.Linear(D, feed_forward_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(feed_forward_dim, feed_forward_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(feed_forward_dim, output_dim) \n",
    "        )\n",
    "    \n",
    "    ff_output = feed_forward(embedded_after_attention) #(Nx2)\n",
    "\n",
    "    #second normalization layer\n",
    "    layer_norm2 = nn.LayerNorm((N,2))\n",
    "    embedded_after_ff = layer_norm2(ff_output)\n",
    "\n",
    "    return embedded_after_ff\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = encoded_input\n",
    "output = transformer(input_sequence,500)\n",
    "output = output[:,:len(seq)]\n",
    "angles_tensor = torch.from_numpy(angles.T)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#calculate loss: softmax --> cross entropy loss\n",
    "loss = criterion(output, angles_tensor)\n",
    "#backward pass and optimize\n",
    "loss.backward()\n",
    "#optimizer.step()\n",
    "total_loss =loss.item()\n",
    "print(total_loss)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
