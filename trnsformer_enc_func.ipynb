{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "\n",
    "seq_example = re.sub(r\"[UZOB]\", \"X\", seq_example)\n",
    "encoded_input = tokenizer(seq_example, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = encoded_input\n",
    "\n",
    "def transformer(input_sequence, feed_forward_dim, output_dim=2):\n",
    "    #putting the input in the embedding space\n",
    "    model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "    output = model(input_sequence)\n",
    "    embedding_prot_bert = output.last_hidden_state\n",
    "    N , D = embedding_prot_bert.size()[1], embedding_prot_bert.size()[2]\n",
    "\n",
    "    #self, single-head, unmasked attention layer\n",
    "    #attention weights\n",
    "    self.w_q = nn.Parameter(torch.randn([D, D])) #(DxD)\n",
    "    self.w_k = nn.Parameter(torch.randn([D, D])) #(DxD)\n",
    "    self.w_v = nn.Parameter(torch.randn([D, D])) #(DxD)\n",
    "\n",
    "    Q = torch.matmul(Q, self.w_q.to(self.device))\n",
    "    K = torch.matmul(K, self.w_k.to(self.device))\n",
    "    V = torch.matmul(V, self.w_v.to(self.device))\n",
    "\n",
    "    scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
    "    #apply the softmax function to obtain the attention weights\n",
    "    attn = torch.softmax(scores, dim=-1)\n",
    "    # Compute the context vector as the weighted sum of the values V\n",
    "    attention_output = torch.matmul(attn, V) #(NxD)\n",
    "\n",
    "    #first normalization layer\n",
    "    layer_norm1 = nn.LayerNorm((N,D))\n",
    "    embedded_after_attention = layer_norm1(attention_output)\n",
    "\n",
    "    #feed forward neural network\n",
    "    feed_forward = nn.Sequential(\n",
    "        nn.Linear(D, feed_forward_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(feed_forward_dim, feed_forward_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(feed_forward_dim, output_dim) \n",
    "        )\n",
    "    \n",
    "    ff_output = feed_forward(embedded_after_attention) #(Nx2)\n",
    "\n",
    "    #second normalization layer\n",
    "    layer_norm2 = nn.LayerNorm((N,D))\n",
    "    embedded_after_ff = layer_norm2(ff_output)\n",
    "\n",
    "    return embedded_after_ff\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
