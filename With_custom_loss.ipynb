{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxLr3K4MCZZa"
      },
      "source": [
        "##Installation of the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOpqGFLLtdTJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqat9Mqke5_y",
        "outputId": "61928d3f-edc7-45e0-d0a2-151a52023109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.83-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.25.2)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.83\n"
          ]
        }
      ],
      "source": [
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dcG5B0v7sNw",
        "outputId": "293fdc1b-2406-407a-9d96-5d78619eca11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "#!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata\n",
        "!pip3 install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH_JLvuAIbf-"
      },
      "source": [
        "Torch optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajkx476vCNeg"
      },
      "source": [
        "##All libraries needed for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdSZUMUnOd6t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import logging\n",
        "\n",
        "# Bring in PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# Most of the examples have typing on the signatures for readability\n",
        "from typing import Optional, Callable, List, Tuple\n",
        "from Bio import SeqIO\n",
        "# For data loading\n",
        "from torch.utils.data import Dataset, IterableDataset, TensorDataset, DataLoader\n",
        "import json\n",
        "import glob\n",
        "import gzip\n",
        "import bz2\n",
        "\n",
        "# For progress and timing\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import shutil\n",
        "from Bio.PDB import PDBList\n",
        "from Bio.PDB.MMCIFParser import MMCIFParser\n",
        "import re\n",
        "\n",
        "#phi and psi\n",
        "from Bio.PDB import PICIO, PDBIO\n",
        "from Bio import PDB\n",
        "from typing import TypedDict, Dict, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLizYwG0Iaf_"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmLGfrATCu2f"
      },
      "source": [
        "##Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OROe4HxxCz93"
      },
      "source": [
        "Getting rid of the sequences which are outside of the threshold (64-128)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxCNl4b7DEjK"
      },
      "source": [
        "Getting the sequence of a given file in the target folder (contains only the files with desired sequences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsWswnTdTXNk",
        "outputId": "b008aeec-e6d9-4c9e-dc1c-f56ea01b2249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MKKLIFVVLLLAVISYTYPSNLDYKYTSSIPIEFVKLSEIKNVDELNRLLNSNALVIFCLDSNNINKDILYHLGIKRYNPSEIPDYGNYSYVSINGVIVVYPKYSVYEENGALIYNPPKEENYSKYEFVRPYKIKVPNVSKIPDYGGYVLIENSTFILYPKKYIIRGDEGGIYYIPPKNSSDNYYYKYSYNLPEKCIPDYYDYVFIDNNGIFIIYPKKYVVRSNDGVLVFSPPVDAKEAPIYKIDYKKIDEGVYEIKKNKLLFLYPTTLNNKSTLDIIGEYIAKNGGVFAYINKVPPYYKHMLATGVAIDKVIPDESGSYAIDVAGRKIKVDVLDDEIINNKLKCIKALKALGINVSYIVTGSEGIDIVEKSNVDTDELEDLYNYYWFKKWWQNYTHLYFNPSQLKNIEFNGFEDILALSYYPLIYVDKAPETFRNDPIGGYYPKVISYKGTKNYGYWEEGAKSENVYYHLDEGEPYWDGKANEPSKWYYEGQPVSMENDSEMWNRYEYFNHWFVKNYAYALANGCDGLFLESSDKNLIDAIFGNDNENLSWKLDIKNKVDYVVIPGNKGFEVINGIPVIRIPSPLKEVYGANVINTLYIPPKDEDFGIYISDIRNYNTNLVVELKENGTDIVSFKDLADWLNKYYRNNIFYNGTAIKIWDNRGIKITIFKKNFGMGNYTFEEFDKEHCKYVIVNPPKIIPLK\n"
          ]
        }
      ],
      "source": [
        "file_path = \"/content/AF-O53113-F1-model_v4.cif\"\n",
        "file_model = \"AF-O53113-F1-model_v4.cif\"\n",
        "pdbl = PDBList()\n",
        "#pdbl.retrieve_pdb_file(file_path, file_format='mmCif', pdir=\".\")\n",
        "# import the needed class\n",
        "# instantiate the class to prepare the parser\n",
        "cif_parser = MMCIFParser()\n",
        "#structure = cif_parser.get_structure(\"3goe\", \"3goe.cif\")\n",
        "structure = cif_parser.get_structure(file_model, file_path)\n",
        "model0 = structure[0]\n",
        "chain_A = model0['A']  # and we get chain A\n",
        "# dictionary converting 3-letter codes to 1-letter codes\n",
        "# this is a very common need in bioinformatics of proteins\n",
        "d3to1 = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',\n",
        " 'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N',\n",
        " 'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W',\n",
        " 'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}\n",
        "\n",
        "sequence = []\n",
        "for residue in chain_A:\n",
        "    # for simplicity we can use X for heteroatoms (ions and water)\n",
        "    sequence.append(d3to1.get(residue.get_resname(), 'X'))  #converts water and ions to X\n",
        "print(''.join(sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPYE1LduF1So"
      },
      "source": [
        "Calculating the angles for the given sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "atom_coords = []\n",
        "\n",
        "for residue in chain_A:\n",
        "\n",
        "    if 'N' in residue and 'CA' in residue and 'C' in residue:\n",
        "\n",
        "        n_coord = residue['N'].get_coord()\n",
        "        ca_coord = residue['CA'].get_coord()\n",
        "        c_coord = residue['C'].get_coord()\n",
        "\n",
        "        atom_coords.append(n_coord)\n",
        "        atom_coords.append(ca_coord)\n",
        "        atom_coords.append(c_coord)\n",
        "\n",
        "        #apparently this is also an extra atom that sometimes exist called Cb so we need to be careful but that will complicate things so for now im ignoring it\n",
        "        #if 'CB' in residue:\n",
        "        #    cb_coord = residue['CB'].get_coord()\n",
        "        #   atom_coords.append(cb_coord)\n",
        "\n",
        "atom_coords = np.array(atom_coords)\n",
        "# Now atom_coords contains the XYZ coordinates of the selected atoms\n",
        "print(atom_coords)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "-zQ2rv0wIYUu",
        "outputId": "2507f95c-5823-40ba-bcee-cd24b66d0288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\natom_coords = []\\n\\nfor residue in chain_A:\\n\\n    if 'N' in residue and 'CA' in residue and 'C' in residue:\\n\\n        n_coord = residue['N'].get_coord()\\n        ca_coord = residue['CA'].get_coord()\\n        c_coord = residue['C'].get_coord()\\n\\n        atom_coords.append(n_coord)\\n        atom_coords.append(ca_coord)\\n        atom_coords.append(c_coord)\\n        \\n        #apparently this is also an extra atom that sometimes exist called Cb so we need to be careful but that will complicate things so for now im ignoring it\\n        #if 'CB' in residue:\\n        #    cb_coord = residue['CB'].get_coord()\\n        #   atom_coords.append(cb_coord)\\n\\natom_coords = np.array(atom_coords)\\n# Now atom_coords contains the XYZ coordinates of the selected atoms\\nprint(atom_coords)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "'''\n",
        "\n",
        "#use this code to update the predicted matrix into pdb file\n",
        "#should have dimensions of atom_coords, im printing it so you can check the dimensions\n",
        "print(np.shape(atom_coords))\n",
        "\n",
        "# Initialize a counter for your new coordinates\n",
        "coord_idx = 0\n",
        "residue_cnt = 0\n",
        "\n",
        "# Loop through each residue and update the coordinates of the N, CA, and C atoms\n",
        "for model in structure:\n",
        "    for chain in model:\n",
        "        for residue in chain:\n",
        "            residue_cnt += 1\n",
        "            if 'N' in residue and 'CA' in residue and 'C' in residue:\n",
        "                # Update N atom\n",
        "                residue['N'].set_coord(atom_coords[coord_idx])\n",
        "                coord_idx += 1\n",
        "                # Update CA atom\n",
        "                residue['CA'].set_coord(atom_coords[coord_idx])\n",
        "                coord_idx += 1\n",
        "                # Update C atom\n",
        "                residue['C'].set_coord(atom_coords[coord_idx])\n",
        "                coord_idx += 1\n",
        "\n",
        "print(residue_cnt)\n",
        "#notice how there are\n",
        "#structure.internal_to_atom_coordinates(verbose = True)\n",
        "io = PDBIO() #this is to write a pdb file again\n",
        "io.set_structure(structure)#set structure, the structure you wan tin the pdb file\n",
        "io.save('xyzcoords.pdb',  preserve_atom_numbering=True)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Rq6MOOSEKiRN",
        "outputId": "dadc496c-e8ed-4518-f1b7-8efee1ef37cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n#use this code to update the predicted matrix into pdb file\\n#should have dimensions of atom_coords, im printing it so you can check the dimensions\\nprint(np.shape(atom_coords))\\n\\n# Initialize a counter for your new coordinates\\ncoord_idx = 0\\nresidue_cnt = 0\\n\\n# Loop through each residue and update the coordinates of the N, CA, and C atoms\\nfor model in structure:\\n    for chain in model:\\n        for residue in chain:\\n            residue_cnt += 1\\n            if 'N' in residue and 'CA' in residue and 'C' in residue:\\n                # Update N atom\\n                residue['N'].set_coord(atom_coords[coord_idx])\\n                coord_idx += 1\\n                # Update CA atom\\n                residue['CA'].set_coord(atom_coords[coord_idx])\\n                coord_idx += 1\\n                # Update C atom\\n                residue['C'].set_coord(atom_coords[coord_idx])\\n                coord_idx += 1\\n\\nprint(residue_cnt)\\n#notice how there are\\n#structure.internal_to_atom_coordinates(verbose = True)\\nio = PDBIO() #this is to write a pdb file again\\nio.set_structure(structure)#set structure, the structure you wan tin the pdb file\\nio.save('xyzcoords.pdb',  preserve_atom_numbering=True)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rHgtHIxGRIK"
      },
      "source": [
        "Putting angles in a matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "delete this later"
      ],
      "metadata": {
        "id": "FEd3SvsHcOIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#phi and psi\n",
        "from Bio.PDB import PICIO, PDBIO\n",
        "from Bio import PDB\n",
        "from typing import TypedDict, Dict, Tuple\n",
        "structure.atom_to_internal_coordinates() # turns xyz coordinates into angles and bond lengths\n",
        "\n",
        "chain:PDB.Chain.Chain = list(structure.get_chains())[0]#iterator of chains, turns it into list, [0] first chain\n",
        "\n",
        "ic_chain: PDB.internal_coords.IC_Chain = chain.internal_coord #this access the internal chain coords of the chain object\n",
        "\n",
        "d: Dict[Tuple[PDB.internal_coords.AtomKey,\n",
        "              PDB.internal_coords.AtomKey,\n",
        "              PDB.internal_coords.AtomKey,\n",
        "              PDB.internal_coords.AtomKey],\n",
        "        PDB.internal_coords.Dihedron] = ic_chain.dihedra\n",
        "\n",
        "cnt = 1\n",
        "phi_angles = {}\n",
        "phi_angles_list = []\n",
        "psi_angles = {}\n",
        "psi_angles_list = []\n",
        "\n",
        "\n",
        "for key in d:\n",
        "    if key[0].akl[3] == 'N' and key[1].akl[3] == 'CA' and key[2].akl[3] == 'C' and key[3].akl[3] == 'N':\n",
        "        phi_angles[key] = d[key].angle\n",
        "        phi_angles_list.append(d[key].angle)\n",
        "    elif key[0].akl[3] == 'CA' and key[1].akl[3] == 'C' and key[2].akl[3] == 'N' and key[3].akl[3] == 'CA':\n",
        "        psi_angles[key] = d[key].angle\n",
        "        psi_angles_list.append(d[key].angle)\n",
        "\n",
        "\n",
        "#structure.internal_to_atom_coordinates(verbose = True)\n",
        "#io = PDBIO() #this is to write a pdb file again\n",
        "#io.set_structure(structure)#set structure, the structure you wan tin the pdb file\n",
        "#io.save('AF-O53113-F1-model_v4_coded.pdb',  preserve_atom_numbering=True)\n",
        "\n",
        "#structure.atom_to_internal_coordinates() # turns xyz coordinates into angles and bond lengths\n",
        "phi_angles_list.append(0)\n",
        "psi_angles_list.append(0)\n",
        "\n",
        "phi = np.asarray(phi_angles_list,dtype=np.float32)*(np.pi/180)\n",
        "psi = np.asarray(psi_angles_list,dtype=np.float32)*(np.pi/180)\n",
        "\n",
        "angles = np.vstack((psi,phi))\n",
        "\n",
        "np.shape(angles)\n",
        "\n",
        "'''\n",
        "cnt = 0\n",
        "\n",
        "\n",
        "for key in d:\n",
        "    if key[0].akl[3] == 'N' and key[1].akl[3] == 'CA' and key[2].akl[3] == 'C' and key[3].akl[3] == 'N':\n",
        "        d[key].angle = angles[0,cnt]\n",
        "\n",
        "\n",
        "    elif key[0].akl[3] == 'CA' and key[1].akl[3] == 'C' and key[2].akl[3] == 'N' and key[3].akl[3] == 'CA':\n",
        "       d[key].angle = angles[1,cnt]\n",
        "       cnt += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "structure.internal_to_atom_coordinates(verbose = True)\n",
        "io = PDBIO() #this is to write a pdb file again\n",
        "io.set_structure(structure)#set structure, the structure you want in the pdb file\n",
        "io.save('input .pdb',  preserve_atom_numbering=True) #saves to a file, filename you a , true - preserves the original atom numbering\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "2bw0VGWXQ_AP",
        "outputId": "69d1b108-62c0-46fd-e156-5e1e12089190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ncnt = 0\\n\\n\\nfor key in d:\\n    if key[0].akl[3] == 'N' and key[1].akl[3] == 'CA' and key[2].akl[3] == 'C' and key[3].akl[3] == 'N':\\n        d[key].angle = angles[0,cnt]\\n\\n        \\n    elif key[0].akl[3] == 'CA' and key[1].akl[3] == 'C' and key[2].akl[3] == 'N' and key[3].akl[3] == 'CA':\\n       d[key].angle = angles[1,cnt] \\n       cnt += 1\\n\\n\\n\\n\\nstructure.internal_to_atom_coordinates(verbose = True)\\nio = PDBIO() #this is to write a pdb file again\\nio.set_structure(structure)#set structure, the structure you want in the pdb file\\nio.save('input .pdb',  preserve_atom_numbering=True) #saves to a file, filename you a , true - preserves the original atom numbering\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEwvvTTpDQdc"
      },
      "source": [
        "Changing sequence for to be used in the Prot-Bert embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "210cltz3TVpu",
        "outputId": "324f34c2-a640-4a22-bc5c-378f523b6666"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'M K K L I F V V L L L A V I S Y T Y P S N L D Y K Y T S S I P I E F V K L S E I K N V D E L N R L L N S N A L V I F C L D S N N I N K D I L Y H L G I K R Y N P S E I P D Y G N Y S Y V S I N G V I V V Y P K Y S V Y E E N G A L I Y N P P K E E N Y S K Y E F V R P Y K I K V P N V S K I P D Y G G Y V L I E N S T F I L Y P K K Y I I R G D E G G I Y Y I P P K N S S D N Y Y Y K Y S Y N L P E K C I P D Y Y D Y V F I D N N G I F I I Y P K K Y V V R S N D G V L V F S P P V D A K E A P I Y K I D Y K K I D E G V Y E I K K N K L L F L Y P T T L N N K S T L D I I G E Y I A K N G G V F A Y I N K V P P Y Y K H M L A T G V A I D K V I P D E S G S Y A I D V A G R K I K V D V L D D E I I N N K L K C I K A L K A L G I N V S Y I V T G S E G I D I V E K S N V D T D E L E D L Y N Y Y W F K K W W Q N Y T H L Y F N P S Q L K N I E F N G F E D I L A L S Y Y P L I Y V D K A P E T F R N D P I G G Y Y P K V I S Y K G T K N Y G Y W E E G A K S E N V Y Y H L D E G E P Y W D G K A N E P S K W Y Y E G Q P V S M E N D S E M W N R Y E Y F N H W F V K N Y A Y A L A N G C D G L F L E S S D K N L I D A I F G N D N E N L S W K L D I K N K V D Y V V I P G N K G F E V I N G I P V I R I P S P L K E V Y G A N V I N T L Y I P P K D E D F G I Y I S D I R N Y N T N L V V E L K E N G T D I V S F K D L A D W L N K Y Y R N N I F Y N G T A I K I W D N R G I K I T I F K K N F G M G N Y T F E E F D K E H C K Y V I V N P P K I I P L K'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "seq_example =  ' '.join(sequence)\n",
        "seq_example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMjT-OTkDaTq"
      },
      "source": [
        "##Embedding space creation (using Prot-Bert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE8wgGamS8q3"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import re\n",
        "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
        "seq = re.sub(r\"[UZOB]\", \"X\", seq_example)\n",
        "encoded_input = tokenizer(seq, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVXU5BycDjBi"
      },
      "source": [
        "Getting the dimensions of the embedding space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpW8wkmfgB84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f4fa83dc-28be-4ce9-c20e-4cf9ef80878a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nN , D = output.last_hidden_state.size()[1], output.last_hidden_state.size()[2]\\nembedding_prot_bert = output.last_hidden_state\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "'''\n",
        "N , D = output.last_hidden_state.size()[1], output.last_hidden_state.size()[2]\n",
        "embedding_prot_bert = output.last_hidden_state\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JlCaJUQZP-6w",
        "outputId": "71154092-cafd-48bf-d24b-d28de26081a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(N)\\nprint(D)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "'''\n",
        "print(N)\n",
        "print(D)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTpKMzbcEdit"
      },
      "source": [
        "##Single-head self unmasked attention layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrauGDe4WeTm"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim: int):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.w_q = nn.Parameter(torch.randn(embed_dim, embed_dim))\n",
        "        self.w_k = nn.Parameter(torch.randn(embed_dim, embed_dim))\n",
        "        self.w_v = nn.Parameter(torch.randn(embed_dim, embed_dim))\n",
        "\n",
        "    def forward(self, embeddings_prot_bert: torch.Tensor) -> torch.Tensor:\n",
        "        Q = torch.matmul(embeddings_prot_bert, self.w_q)\n",
        "        K = torch.matmul(embeddings_prot_bert, self.w_k)\n",
        "        V = torch.matmul(embeddings_prot_bert, self.w_v)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(K.size(-1))\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attention_output = torch.matmul(attn, V)\n",
        "        return attention_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-byNjH-FAbp"
      },
      "source": [
        "##Encoder with attention and 2 layer FFNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh9FkMFIff6L"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, embed_dim: int, feed_forward_dim1: int, feed_forward_dim2: int, output_dim: int = 2):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.self_attention = SelfAttention(embed_dim)\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(output_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, feed_forward_dim1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feed_forward_dim1, feed_forward_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feed_forward_dim2, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
        "        attention_output = self.self_attention(embeddings)\n",
        "        normalized_attention_output = self.layer_norm1(attention_output)\n",
        "        ff_output = self.feed_forward(normalized_attention_output)\n",
        "        #output = self.layer_norm2(ff_output)\n",
        "        return ff_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wna5fDOhP-6w"
      },
      "outputs": [],
      "source": [
        "class TransformerTrainer:\n",
        "    def __init__(self, model: nn.Module, criterion: nn.Module, num_epochs: int, sequence: torch.Tensor, angles: np.ndarray):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.num_epochs = num_epochs\n",
        "        self.sequence = sequence\n",
        "        self.angles_tensor = torch.from_numpy(angles.T)\n",
        "        self.optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
        "        #self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "    def train(self):\n",
        "        loss_list = []\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.optimizer.zero_grad()\n",
        "            predictions = self.model.forward(self.sequence)[:, :len(self.angles_tensor)]\n",
        "            loss = self.criterion(predictions.squeeze(), self.angles_tensor)\n",
        "            loss.backward(retain_graph=True)\n",
        "            self.optimizer.step()\n",
        "            #self.scheduler.step()\n",
        "            loss_list.append(loss.item())\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{self.num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with using the mse architecture idea\n",
        "class TransformerTrainer2:\n",
        "    def __init__(self, model: nn.Module, criterion: nn.Module, num_epochs: int, sequence: torch.Tensor, angles: np.ndarray):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.num_epochs = num_epochs\n",
        "        self.sequence = sequence\n",
        "        self.angles_tensor = torch.from_numpy(angles.T)\n",
        "        self.optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
        "        #self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.1)\n",
        "        self.empty_tensor = torch.from_numpy(np.zeros(np.shape(angles.T)))\n",
        "\n",
        "    def train(self):\n",
        "        loss_list = []\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.optimizer.zero_grad()\n",
        "            predictions = self.model.forward(self.sequence)[:, :len(self.angles_tensor)]\n",
        "\n",
        "            #loss = self.criterion(predictions.squeeze(), self.angles_tensor)\n",
        "            loss = self.criterion(MeanAngleErrorLoss(predictions,self.angles_tensor).squeeze(), self.empty_tensor)\n",
        "\n",
        "            loss.backward(retain_graph=True)\n",
        "            self.optimizer.step()\n",
        "            #self.scheduler.step()\n",
        "            loss_list.append(loss.item())\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{self.num_epochs}, Loss: {loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tYn52SFM0BSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MeanAngleErrorLoss(nn.Module):\n",
        "  def __init__(self, predictions, angles):\n",
        "        super(MeanAngleErrorLoss, self).__init__()\n",
        "  def forward(self, predictions, angles):\n",
        "    '''\n",
        "    predictions_np = predictions.detach().cpu().numpy()#turning it to numpy arrays\n",
        "    angles_np = angles.detach().cpu().numpy()\n",
        "\n",
        "    angular_diff = np.abs(predictions_np- angles_np)\n",
        "    alt_diff = 2*np.pi - angular_diff\n",
        "\n",
        "    print(self.shape)\n",
        "    error_phi = []\n",
        "    error_psi = []\n",
        "    for i in range(len(predictions_np[:,0])):\n",
        "       angular_diff = np.abs(predictions_np[i,0]- angles_np[i,0])\n",
        "       alt_diff = 360 - angular_diff\n",
        "       error_phi.append(min(angular_diff, alt_diff))\n",
        "       angular_diff = np.abs(predictions_np[i,1]- angles_np[i,1])\n",
        "       alt_diff = 360 - angular_diff\n",
        "       error_psi.append(min(angular_diff, alt_diff))\n",
        "\n",
        "    error_phi = np.array(error_phi)\n",
        "    error_psi = np.array(error_psi)\n",
        "\n",
        "    error_combined = np.column_stack((error_phi, error_psi))\n",
        "    return torch.from_numpy(error_combined)\n",
        "    '''\n",
        "    angular_diff_phi = torch.abs(predictions[:, 0] - angles[:, 0])\n",
        "    alt_diff_phi = 2*torch.pi - angular_diff_phi\n",
        "    error_phi = torch.min(angular_diff_phi, alt_diff_phi)\n",
        "\n",
        "    angular_diff_psi = torch.abs(predictions[:, 1] - angles[:, 1])\n",
        "    alt_diff_psi = 2*torch.pi - angular_diff_psi\n",
        "    error_psi = torch.min(angular_diff_psi, alt_diff_psi)\n",
        "\n",
        "    error_combined = torch.stack((error_phi, error_psi), dim=1)\n",
        "    return error_combined\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6rSOnrgJap6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I9Yejd9Czq7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "angles.T[:, 0].shape\n",
        "angles"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0teahi_DlFZ",
        "outputId": "f5e69ce7-be9d-4db1-eb3b-65ee1fe68d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-3.0019474 ,  3.088406  ,  3.1186235 , ..., -3.1220431 ,\n",
              "         3.0670881 ,  0.        ],\n",
              "       [-1.225698  , -0.8935808 , -0.5575172 , ...,  2.4149952 ,\n",
              "         0.32593784,  0.        ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerTrainer2:\n",
        "    def __init__(self, model: nn.Module, criterion: nn.Module, num_epochs: int, sequence: torch.Tensor, angles: np.ndarray):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.num_epochs = num_epochs\n",
        "        self.sequence = sequence\n",
        "        self.angles_tensor = torch.from_numpy(angles.T)\n",
        "        self.optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
        "\n",
        "    def train(self):\n",
        "        loss_list = []\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.optimizer.zero_grad()\n",
        "            predictions = self.model.forward(self.sequence)[:, :len(self.angles_tensor)]\n",
        "\n",
        "            loss = self.criterion(predictions.squeeze(), self.angles_tensor)\n",
        "\n",
        "\n",
        "            loss.backward(retain_graph=True)\n",
        "            self.optimizer.step()\n",
        "            loss_list.append(loss.item())\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{self.num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "#with mea scalar output\n",
        "class MeanAngleErrorLoss(nn.Module):\n",
        "    def __init__(self, predictions, angles):\n",
        "        super(MeanAngleErrorLoss, self).__init__()\n",
        "\n",
        "    def forward(self, predictions, angles):\n",
        "        angular_diff_phi = torch.abs(predictions[:, 0] - angles[:, 0])\n",
        "        alt_diff_phi = 2*torch.pi - angular_diff_phi\n",
        "        error_phi = torch.min(angular_diff_phi, alt_diff_phi)\n",
        "\n",
        "        angular_diff_psi = torch.abs(predictions[:, 1] - angles[:, 1])\n",
        "        alt_diff_psi = 2*torch.pi - angular_diff_psi\n",
        "        error_psi = torch.min(angular_diff_psi, alt_diff_psi)\n",
        "\n",
        "        error_combined = torch.stack((error_phi, error_psi), dim=1)\n",
        "        #mse = torch.mean(error_combined ** 2)\n",
        "        return error_combined\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I89eAdC4Ee5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with  using mean angle error loss directly\n",
        "class TransformerTrainer2:\n",
        "    def __init__(self, model: nn.Module, criterion: nn.Module, num_epochs: int, sequence: torch.Tensor, angles: np.ndarray):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.num_epochs = num_epochs\n",
        "        self.sequence = sequence\n",
        "        self.angles_tensor = torch.from_numpy(angles.T)\n",
        "        self.optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
        "\n",
        "    def train(self):\n",
        "        loss_list = []\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.optimizer.zero_grad()\n",
        "            predictions = self.model.forward(self.sequence)[:, :len(self.angles_tensor)]\n",
        "\n",
        "            loss = self.criterion(predictions.squeeze(), self.angles_tensor)\n",
        "\n",
        "\n",
        "            loss.backward(retain_graph=True)\n",
        "            self.optimizer.step()\n",
        "            loss_list.append(loss.item())\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{self.num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "class MeanAngleErrorLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MeanAngleErrorLoss, self).__init__()\n",
        "\n",
        "    def forward(self, predictions, angles):\n",
        "\n",
        "        angular_diff_phi = torch.abs(predictions[:, 0] - angles[:, 0])\n",
        "        alt_diff_phi = 2 * torch.pi - angular_diff_phi\n",
        "        error_phi = torch.min(angular_diff_phi, alt_diff_phi)\n",
        "        #print(predictions)\n",
        "\n",
        "        angular_diff_psi = torch.abs(predictions[:, 1] - angles[:, 1])\n",
        "        alt_diff_psi = 2 * torch.pi - angular_diff_psi\n",
        "        error_psi = torch.min(angular_diff_psi, alt_diff_psi)\n",
        "\n",
        "\n",
        "        error_combined = torch.stack((error_phi, error_psi), dim=1)\n",
        "        #print(np.shape(error_combined))\n",
        "        mse = torch.mean(error_combined ** 2)\n",
        "        #maybe we can use squeeze here\n",
        "        return mse"
      ],
      "metadata": {
        "id": "2guB8Iu-R9XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with  using mean angle error loss with mse loss\n",
        "class TransformerTrainer3:\n",
        "    def __init__(self, model: nn.Module, criterion: nn.Module, num_epochs: int, sequence: torch.Tensor, angles: np.ndarray):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.num_epochs = num_epochs\n",
        "        self.sequence = sequence\n",
        "        self.angles_tensor = torch.from_numpy(angles.T).float()\n",
        "        self.optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
        "        self.empty_tensor = torch.zeros_like(self.angles_tensor)\n",
        "\n",
        "    def train(self):\n",
        "        loss_list = []\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.optimizer.zero_grad()\n",
        "            predictions = self.model.forward(self.sequence)[:, :len(self.angles_tensor)]\n",
        "            mean_angle_error = MeanAngleErrorLoss2()(predictions.squeeze(), self.angles_tensor)\n",
        "            loss = self.criterion(mean_angle_error, self.empty_tensor)\n",
        "\n",
        "\n",
        "            loss.backward(retain_graph=True)\n",
        "            self.optimizer.step()\n",
        "            loss_list.append(loss.item())\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{self.num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "class MeanAngleErrorLoss2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MeanAngleErrorLoss2, self).__init__()\n",
        "\n",
        "\n",
        "    def forward(self, predictions, angles):\n",
        "\n",
        "        angular_diff_phi = torch.abs(predictions[:, 0] -angles[:, 0])\n",
        "        alt_diff_phi = 2 * torch.pi - angular_diff_phi\n",
        "        error_phi = torch.min(angular_diff_phi, alt_diff_phi)\n",
        "\n",
        "        angular_diff_psi = torch.abs(predictions[:, 1] - angles[:, 1])\n",
        "        alt_diff_psi = 2 * torch.pi - angular_diff_psi\n",
        "        error_psi = torch.min(angular_diff_psi, alt_diff_psi)\n",
        "\n",
        "        error_combined = torch.stack((error_phi, error_psi), dim=1)\n",
        "        #print(np.shape(error_combined))\n",
        "        #mse = torch.mean(error_combined ** 2)\n",
        "        #maybe we can use squeeze here\n",
        "        return error_combined"
      ],
      "metadata": {
        "id": "FIeMEJl9PuzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "2ilZnbZAP-6x",
        "outputId": "49442acc-2dfc-4638-fccd-c9bbcd61ff67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmodel = TransformerModel(embed_dim=D, feed_forward_dim1=feed_forward_dim1, feed_forward_dim2= feed_forward_dim2)\\n#model.forward(embeddings)\\n#criterion = MeanAngleErrorLoss()\\ncriterion = nn.MSELoss()\\n#criterion = nn.L1Loss()\\ntrainer = TransformerTrainer(model, criterion, num_epochs, embedded_pb, angles)\\ntrainer.train()\\nmodel\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "bert_model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
        "output = bert_model(**encoded_input)\n",
        "embedded_pb = output.last_hidden_state\n",
        "N, D = embedded_pb.size()[1], embedded_pb.size()[2]\n",
        "embeddings = embedded_pb\n",
        "# Initialize and train transformer model\n",
        "feed_forward_dim1 = 512\n",
        "feed_forward_dim2 = 256\n",
        "feed_forward_dim3 = 128\n",
        "num_epochs = 20\n",
        "\n",
        "'''\n",
        "model = TransformerModel(embed_dim=D, feed_forward_dim1=feed_forward_dim1, feed_forward_dim2= feed_forward_dim2)\n",
        "#model.forward(embeddings)\n",
        "#criterion = MeanAngleErrorLoss()\n",
        "criterion = nn.MSELoss()\n",
        "#criterion = nn.L1Loss()\n",
        "trainer = TransformerTrainer(model, criterion, num_epochs, embedded_pb, angles)\n",
        "trainer.train()\n",
        "model\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = TransformerModel(embed_dim=D, feed_forward_dim1=feed_forward_dim1, feed_forward_dim2= feed_forward_dim2)\n",
        "#model.forward(embeddings)\n",
        "#criterion = MeanAngleErrorLoss()\n",
        "criterion = nn.MSELoss()\n",
        "#criterion = nn.L1Loss()\n",
        "trainer = TransformerTrainer3(model, criterion, num_epochs, embedded_pb, angles)\n",
        "trainer.train()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3G1UUDdOucZ",
        "outputId": "a19c36fd-2485-4683-ddd2-07efafb80a0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 2.2573182582855225\n",
            "Epoch 2/20, Loss: 2.2323081493377686\n",
            "Epoch 3/20, Loss: 2.194566011428833\n",
            "Epoch 4/20, Loss: 2.148908853530884\n",
            "Epoch 5/20, Loss: 2.0980870723724365\n",
            "Epoch 6/20, Loss: 2.043680429458618\n",
            "Epoch 7/20, Loss: 1.9868818521499634\n",
            "Epoch 8/20, Loss: 1.927472472190857\n",
            "Epoch 9/20, Loss: 1.8650704622268677\n",
            "Epoch 10/20, Loss: 1.7980220317840576\n",
            "Epoch 11/20, Loss: 1.7259459495544434\n",
            "Epoch 12/20, Loss: 1.647971272468567\n",
            "Epoch 13/20, Loss: 1.5624860525131226\n",
            "Epoch 14/20, Loss: 1.4680578708648682\n",
            "Epoch 15/20, Loss: 1.3635672330856323\n",
            "Epoch 16/20, Loss: 1.2471768856048584\n",
            "Epoch 17/20, Loss: 1.1168537139892578\n",
            "Epoch 18/20, Loss: 0.9696646332740784\n",
            "Epoch 19/20, Loss: 0.8181232213973999\n",
            "Epoch 20/20, Loss: 0.7960692048072815\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (self_attention): SelfAttention()\n",
              "  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  (layer_norm2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
              "  (feed_forward): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02HNhrf1P-6x"
      },
      "outputs": [],
      "source": [
        "\n",
        "angles_tensor = torch.from_numpy(angles.T)\n",
        "\n",
        "predicted_angles = model.forward(embedded_pb)[:, :len(angles_tensor)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVEf-3s5P-6x",
        "outputId": "90b7f910-ca31-4f3e-97af-37fc4ea34504"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-181.3975,   67.4064],\n",
              "         [-188.3036,   69.5110],\n",
              "         [-167.0988,   63.8460],\n",
              "         ...,\n",
              "         [-186.5330,   69.1219],\n",
              "         [-175.2244,   71.4057],\n",
              "         [-186.2243,   69.2317]]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "pred = predicted_angles*(180/np.pi)\n",
        "pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOKEZMYqP-6x"
      },
      "outputs": [],
      "source": [
        "angles_tensor = angles_tensor*(180/np.pi)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJTdDSpHWgvS",
        "outputId": "c7fcc043-53a3-43b3-d5d2-7263fde429b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 703, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "angles_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd8ueWzxwC1T",
        "outputId": "ae45220f-4906-4ad5-f918-2ca2a899d444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-171.9989,  -70.2273],\n",
              "        [ 176.9526,  -51.1984],\n",
              "        [ 178.6840,  -31.9434],\n",
              "        ...,\n",
              "        [-178.8799,  138.3690],\n",
              "        [ 175.7312,   18.6749],\n",
              "        [   0.0000,    0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqv5T21sP-6x",
        "outputId": "a1087b1d-98e5-4871-ee5d-e0bcaf871203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AF-O53113-F1-model_v4.cif A coordinates for 5786 dihedra updated in 2112 iterations\n"
          ]
        }
      ],
      "source": [
        "structure.atom_to_internal_coordinates() # turns xyz coordinates into angles and bond lengths\n",
        "\n",
        "chain:PDB.Chain.Chain = list(structure.get_chains())[0]#iterator of chains, turns it into list, [0] first chain\n",
        "\n",
        "ic_chain: PDB.internal_coords.IC_Chain = chain.internal_coord #this access the internal chain coords of the chain object\n",
        "#if you modify this, you will modify the orgiginal\n",
        "\n",
        "d: Dict[Tuple[PDB.internal_coords.AtomKey,\n",
        "              PDB.internal_coords.AtomKey,\n",
        "              PDB.internal_coords.AtomKey,\n",
        "              PDB.internal_coords.AtomKey],\n",
        "        PDB.internal_coords.Dihedron] = ic_chain.dihedra\n",
        "\n",
        "cnt = 0\n",
        "phi_angles = {}\n",
        "psi_angles = {}\n",
        "\n",
        "for key in d:\n",
        "    if key[0].akl[3] == 'N' and key[1].akl[3] == 'CA' and key[2].akl[3] == 'C' and key[3].akl[3] == 'N':\n",
        "\n",
        "        d[key].angle = pred[0,cnt, 0].item()\n",
        "        #print('phi clculated')\n",
        "    elif key[0].akl[3] == 'CA' and key[1].akl[3] == 'C' and key[2].akl[3] == 'N' and key[3].akl[3] == 'CA':\n",
        "        d[key].angle = pred[0,cnt, 1].item()\n",
        "        #print(\"psi calcukated\")\n",
        "        #print(cnt)\n",
        "        cnt += 1\n",
        "\n",
        "\n",
        "\n",
        "structure.internal_to_atom_coordinates(verbose = True)\n",
        "io = PDBIO() #this is to write a pdb file again\n",
        "io.set_structure(structure)#set structure, the structure you want in the pdb file\n",
        "io.save('pred .pdb',  preserve_atom_numbering=True) #saves to a file, filename you a , true - preserves the original atom numbering\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JHdmTahbeQGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zstructure.atom_to_internal_coordinates() # turns xyz coordinates into angles and bond lengths\n",
        "\n",
        "chain:PDB.Chain.Chain = list(structure.get_chains())[0]#iterator of chains, turns it into list, [0] first chain\n",
        "\n",
        "ic_chain: PDB.internal_coords.IC_Chain = chain.internal_coord #this access the internal chain coords of the chain object\n",
        "#if you modify this, you will modify the orgiginal\n",
        "\n",
        "d: Dict[Tuple[PDB.internal_coords.AtomKey,\n",
        "              PDB.internal_coords.AtomKey,\n",
        "              PDB.internal_coords.AtomKey,\n",
        "              PDB.internal_coords.AtomKey],\n",
        "        PDB.internal_coords.Dihedron] = ic_chain.dihedra\n",
        "\n",
        "cnt = 0\n",
        "phi_angles = {}\n",
        "psi_angles = {}\n",
        "\n",
        "for key in d:\n",
        "    if key[0].akl[3] == 'N' and key[1].akl[3] == 'CA' and key[2].akl[3] == 'C' and key[3].akl[3] == 'N':\n",
        "\n",
        "        d[key].angle = angles_tensor[0,cnt].item()\n",
        "        #print('phi clculated')\n",
        "    elif key[0].akl[3] == 'CA' and key[1].akl[3] == 'C' and key[2].akl[3] == 'N' and key[3].akl[3] == 'CA':\n",
        "        d[key].angle = angles_tensor[1,cnt].item()\n",
        "        #print(\"psi calcukated\")\n",
        "        #print(cnt)\n",
        "        cnt += 1\n",
        "\n",
        "\n",
        "\n",
        "structure.internal_to_atom_coordinates(verbose = True)\n",
        "io = PDBIO() #this is to write a pdb file again\n",
        "io.set_structure(structure)#set structure, the structure you want in the pdb file\n",
        "io.save('AF-O06917-F1-model_v4_amnglechanged .pdb',  preserve_atom_numbering=True) #saves to a file, filename you a , true - preserves the original atom numbering\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "NUO6AG5zWVGN",
        "outputId": "a10b2aca-c129-4c99-acfe-ffe7a659f8b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'zstructure' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-c61cf96555b2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mzstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matom_to_internal_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# turns xyz coordinates into angles and bond lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mPDB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#iterator of chains, turns it into list, [0] first chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mic_chain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPDB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_coords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIC_Chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_coord\u001b[0m \u001b[0;31m#this access the internal chain coords of the chain object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'zstructure' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utUIUxseLB4G"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}