# Transformer From Scratch

## starting ideas
maybe we can start with a (örüntü) and it can try to grasp the idea behind the pattern

## Architecture structures
- [ ] Encoder decoder ?
- [ ] Embedding space
- [ ] Encoding
- [ ] Attention
- [ ] Feed-forward neural network
- [ ] Add & Norm (?)
- [ ] Linearization
- [ ] Softmax

## Architecture design

## 

## Goals
- [ ] Interpretability
- [ ] Stabile results
- [ ] Minimal

## Training
The embedding space creation using tokenization can be done via the mapd project. (?)
Training can be done via a distributed system architecture where the pattern is already known so the data is simulated. \
By increasing the complexity of the transformer the data can have more complex patterns as well.





University of Padova \
Laboratory of Computational Physics Project \
Supervisor Prof. Jeff Byers
