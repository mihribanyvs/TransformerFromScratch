# Transformer From Scratch

## Architecture structures
- [ ] Embedding space
- [ ] Encoding
- [ ] Attention
- [ ] Feed-forward neural network
- [ ] Add & Norm (?)
- [ ] Linearization
- [ ] Softmax

## Architecture design

## 

## Goals
- [ ] Interpretability
- [ ] Stabile results
- [ ] Minimal

## Training
Training can be done via a distributed system architecture where the pattern is already known so the data is simulated. \
By increasing the complexity of the transformer the data can have more complex patterns as well.





University of Padova 
Laboratory of Computational Physics Project
Supervisor Prof. Jeff Byers
